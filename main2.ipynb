{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3d0f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from scipy.spatial.distance import cosine\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b08398",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('attendance.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS attendance (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT,\n",
    "    timestamp TEXT\n",
    ")\n",
    "''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cb01fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True)\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4027b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veritabanı oluşturuldu: dict_keys(['abdullatif_akkaya', 'ahmet_furkan_aylac', 'ahmet_kadir_cicek', 'alperen_gozum', 'alper_isleyen', 'emre_salaman', 'enes_berk_demirci', 'gokhan_guney', 'oguzhan_cinar', 'serhat_derya', 'seyma_hacifettahoglu'])\n"
     ]
    }
   ],
   "source": [
    "known_faces = {} \n",
    "dataset_path = 'dataset/' \n",
    "\n",
    "for person_name in os.listdir(dataset_path):\n",
    "    person_path = os.path.join(dataset_path, person_name)\n",
    "    if os.path.isdir(person_path):\n",
    "        embeddings = []  \n",
    "        for image_name in os.listdir(person_path):\n",
    "            image_path = os.path.join(person_path, image_name)\n",
    "            if image_path.endswith('.jpg'):\n",
    "                img = Image.open(image_path)\n",
    "                img_rgb = img.convert('RGB')\n",
    "                faces = mtcnn(img_rgb)\n",
    "                if faces is not None:\n",
    "                    face_tensor = faces[0]\n",
    "                    face_tensor = face_tensor.unsqueeze(0)  \n",
    "                    embedding = model(face_tensor).detach().numpy()\n",
    "                    embeddings.append(embedding)\n",
    "        if embeddings:\n",
    "            known_faces[person_name] = np.mean(embeddings, axis=0)\n",
    "\n",
    "print(\"Veritabanı oluşturuldu:\", known_faces.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alper_isleyen için yoklama kaydedildi: 2025-05-19 13:20:54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m     53\u001b[39m pil_frame = Image.fromarray(frame_rgb)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m boxes, _ = \u001b[43mmtcnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     draw = ImageDraw.Draw(pil_frame)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[39m, in \u001b[36mMTCNN.detect\u001b[39m\u001b[34m(self, img, landmarks)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m \u001b[33;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     batch_boxes, batch_points = \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m boxes, probs, points = [], [], []\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:146\u001b[39m, in \u001b[36mdetect_face\u001b[39m\u001b[34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[39m\n\u001b[32m    143\u001b[39m im_data = (im_data - \u001b[32m127.5\u001b[39m) * \u001b[32m0.0078125\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# This is equivalent to out = onet(im_data) to avoid GPU out of memory.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m out = \u001b[43mfixed_batch_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m out0 = out[\u001b[32m0\u001b[39m].permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    149\u001b[39m out1 = out[\u001b[32m1\u001b[39m].permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:21\u001b[39m, in \u001b[36mfixed_batch_process\u001b[39m\u001b[34m(im_data, model)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(im_data), batch_size):\n\u001b[32m     20\u001b[39m     batch = im_data[i:(i+batch_size)]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     out.append(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(torch.cat(v, dim=\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*out))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\facenet_pytorch\\models\\mtcnn.py:137\u001b[39m, in \u001b[36mONet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    136\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprelu1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool1(x)\n\u001b[32m    139\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\activation.py:1364\u001b[39m, in \u001b[36mPReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def show_attendance():\n",
    "    cursor.execute('SELECT name, timestamp FROM attendance ORDER BY timestamp DESC')\n",
    "    records = cursor.fetchall()\n",
    "    print(\"\\nYoklama Listesi:\")\n",
    "    print(\"Name\\t\\t\\tTimestamp\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, timestamp in records:\n",
    "        print(f\"{name}\\t\\t{timestamp}\")\n",
    "\n",
    "def compare_faces(embedding, known_faces):\n",
    "    min_distance = float('inf')\n",
    "    name = None\n",
    "    for known_name, known_embedding in known_faces.items():\n",
    "        distance = cosine(embedding.flatten(), known_embedding.flatten()) \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            name = known_name\n",
    "    if min_distance < 0.3: \n",
    "        return name\n",
    "    else:\n",
    "        return \"Tanımlanamayan Kişi\"\n",
    "\n",
    "recorded_names = set()\n",
    "\n",
    "def add_attendance(name):\n",
    "    if name in recorded_names:\n",
    "        return\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    cursor.execute('INSERT INTO attendance (name, timestamp) VALUES (?, ?)', (name, current_time))\n",
    "    conn.commit()\n",
    "    recorded_names.add(name)\n",
    "    print(f\"{name} için yoklama kaydedildi: {current_time}\")\n",
    "\n",
    "verification_counts = defaultdict(int)\n",
    "verification_threshold = 5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Kamera açılamadı!\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    prev_time = time.time()\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Kare okunamadı!\")\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_frame = Image.fromarray(frame_rgb)\n",
    "\n",
    "        boxes, _ = mtcnn.detect(pil_frame)\n",
    "        if boxes is not None:\n",
    "            draw = ImageDraw.Draw(pil_frame)\n",
    "            for box in boxes:\n",
    "                draw.rectangle(box.tolist(), outline=(255, 0, 0), width=3)\n",
    "                face = pil_frame.crop((box[0], box[1], box[2], box[3]))\n",
    "                faces = mtcnn(face) \n",
    "                if faces is not None:\n",
    "                    face_embedding = model(faces[0].unsqueeze(0)).detach().numpy()  \n",
    "                    name = compare_faces(face_embedding, known_faces)\n",
    "\n",
    "                    font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "                    draw.text((box[0], box[1] - 30), name, fill=(255, 0, 0), font=font)\n",
    "\n",
    "                    if name != \"Tanımlanamayan Kişi\":\n",
    "                        verification_counts[name] += 1\n",
    "                        if verification_counts[name] >= verification_threshold:\n",
    "                            add_attendance(name)\n",
    "                            verification_counts[name] = 0\n",
    "                    else:\n",
    "                        verification_counts = defaultdict(int)\n",
    "\n",
    "        frame_with_boxes = cv2.cvtColor(np.array(pil_frame), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        curr_time = time.time()\n",
    "        fps = 1 / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "\n",
    "        cv2.putText(frame_with_boxes, f\"FPS: {fps:.2f}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Face Recognition', frame_with_boxes)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('r'):\n",
    "            show_attendance()\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
